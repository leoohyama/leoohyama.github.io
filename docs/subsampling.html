<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>subsampling.utf8</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/darkly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>





<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 60px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h2 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h3 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h4 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h5 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h6 {
  padding-top: 65px;
  margin-top: -65px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Ohyama</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="CV.html">CV</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Research
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="macroecology.html">Macroecology</a>
    </li>
    <li>
      <a href="SAR.html">Island Biogeography</a>
    </li>
    <li>
      <a href="thiefant.html">Community Ecology</a>
    </li>
    <li>
      <a href="readings-module4.html">Non-ants</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    R stuff
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="NYCflights.html">Data Exploration</a>
    </li>
    <li>
      <a href="maps.html">Global Maps</a>
    </li>
    <li>
      <a href="subsampling.html">Subsampling at the global scale</a>
    </li>
    <li>
      <a href="advancedR.html">Tidyverse Functions</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">




</div>


<div id="subsampling" class="section level1">
<h1>Subsampling</h1>
<p>When analyzing data at large spatial scales, significant results of correlations between your response variable and predictor variable need to be better validated aside from usual metrics such as correlation coefficients. While cross-validation methods exist to validate the predictive power of a statistical analysis, such as a regression, sometimes the question that we need to ask is whether or not the pattern or correlation that is observed from the analysis is really there.</p>
<p>Take for example a study where we run a model assessing the effects of temperature range on the average assemblage body size of beetles. Our model shows a significant effect and a decent correlation coefficient, however there is a possibility that our results are a product of certain species influencing the data because they are more abundant or widespread. Now the question becomes whether our model is simply just capturing the variation that is contributed by only several abundant species. How do we test this?</p>
<p>Random sampling is an efficient approach to solving this issue. By randomly sampling a number of species (usually the minimum number of species found in a assemblage) from each assemblage (without replacement, meaning the same species isn’t sampled twice) we pull a less biased species roster as such a less biased value of body size.</p>
<p>Here I’ll show how to code a simple program in R to run subsampling.</p>
</div>
<div id="subsampling-in-r" class="section level1">
<h1>Subsampling in R</h1>
<p>Let’s load in our data. In this case we will use hexbin shapefiles.</p>
<pre class="r"><code>#packages
library(sf)
library(tidyverse)
library(nlme)
library(viridis)
library(lwgeom)</code></pre>
<p>This is how you read in your shapefile data:</p>
<pre class="r"><code>df_shp &lt;- st_read(
  &quot;~/Desktop/hexbin_wenviron/hexbin_wenviron.shp&quot;)</code></pre>
<pre><code>## Reading layer `hexbin_wenviron&#39; from data source `/Users/leoohyama/Desktop/hexbin_wenviron/hexbin_wenviron.shp&#39; using driver `ESRI Shapefile&#39;
## Simple feature collection with 464 features and 10 fields
## geometry type:  MULTIPOLYGON
## dimension:      XY
## bbox:           xmin: -177.958 ymin: -60.77272 xmax: 180 ymax: 83.6341
## geographic CRS: WGS 84</code></pre>
<pre class="r"><code>#Let&#39;s look at the data
df_shp</code></pre>
<pre><code>## Simple feature collection with 464 features and 10 fields
## geometry type:  MULTIPOLYGON
## dimension:      XY
## bbox:           xmin: -177.958 ymin: -60.77272 xmax: 180 ymax: 83.6341
## geographic CRS: WGS 84
## First 10 features:
##    cell SR  geo_avg         MAT         CM       MAP       WET      ATR
## 1     1 56 1.091213   4.7616038  -7.087847  900.9742 300.04575 26.58891
## 2     2 NA       NA  -0.7031792 -15.135383  638.4850 297.98941 30.03473
## 3     3 NA       NA  -6.1747150 -22.750118  491.6231 190.38200 36.20150
## 4     4 NA       NA -12.5492020 -29.791090  146.9178  70.94612 34.76204
## 5     8 NA       NA  -8.4239092 -19.696692  446.2876 142.48109 22.76368
## 6     9  7 1.688046   1.1910487  -9.261226  861.5570 276.14386 23.71292
## 7    10 50 1.209047   0.9827181 -12.573601  832.6850 286.08417 29.55614
## 8    11 NA       NA   2.6216834  -5.239853 1059.2101 371.68143 18.05475
## 9    12 15 1.212341  -0.6845881 -17.013023  606.3929 253.82622 34.34307
## 10   13 22 1.096853  -7.2203846 -28.350182  262.0726 130.85980 46.19601
##         NPP       TREE                       geometry
## 1  550.4055 54513.9180 MULTIPOLYGON (((8.674604 54...
## 2  385.7403 11046.1836 MULTIPOLYGON (((-162.1393 5...
## 3        NA  8948.4346 MULTIPOLYGON (((180 65.0922...
## 4    0.0000   418.3625 MULTIPOLYGON (((178.8547 71...
## 5        NA   314.5493 MULTIPOLYGON (((27.02076 78...
## 6  332.4632 37050.3672 MULTIPOLYGON (((17.5353 68....
## 7  420.8561 50186.0430 MULTIPOLYGON (((21.89008 65...
## 8  420.8124 21194.8848 MULTIPOLYGON (((-158.6881 5...
## 9  418.0304 27562.4551 MULTIPOLYGON (((-148.919 59...
## 10 287.1639 17601.9375 MULTIPOLYGON (((-154.5998 7...</code></pre>
<p>The data that was read in is a shapefile of the world divided into hexbins. Each row represent a hexbin polygon and there is information for different attributes for each of these hexbins:</p>
<ul>
<li>cell: hexbin cell identifier</li>
<li>SR: species richness within a hexbin</li>
<li>geo_avg: average size metric of all species inside that hexbin</li>
<li>MAT: Mean annual temperature</li>
<li>CM: Coldest month temperature</li>
<li>MAP: Mean annual precipitation</li>
<li>WET: Temperature of wettest month</li>
<li>ATR: Annual temperature range</li>
<li>NPP: Net primary productivity</li>
<li>TREE: Tree density</li>
</ul>
<p><strong>Note:</strong> All data was averaged per hexbin</p>
<p>In this case we consider a hexbin as a single assemblage as such the values per hexbin represent average values for each assemblage. We will also remove any assemblages with less than 5 species to set a biologically reasonable minimum species richness.</p>
<p>A quick plot of this shows the distribution of body size across the world:</p>
<pre class="r"><code>df_shp_master&lt;-df_shp #make a master copy in case we need it
df_shp &lt;- df_shp %&gt;% filter(!SR&lt;5)

ggplot() +
  geom_sf(data = df_shp_master, alpha = 0.7) +
  geom_sf(data = df_shp, aes(fill = geo_avg, color = geo_avg)) +
  scale_fill_viridis_c()+
  scale_color_viridis_c()+
  labs(fill = &quot;Body Size&quot;, color = &quot;Body Size&quot;) +
  theme(panel.background = element_rect(fill = &quot;black&quot;, colour = NA),
        panel.grid = element_blank())</code></pre>
<p><img src="subsampling_files/figure-html/unnamed-chunk-3-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>Now let’s a build a model where we assess geo_avg (our variable for body size) as a function of ATR. We also need a model to account for spatial autocorrelation. We will use generalized least squares (GLS) regression as it permits analysis of data given the assumption that there is a high level or correlation between our residuals (a typical sign of spatial autocorrelation). Finally the model should also account for the species richness of each assemblage because more species may influence the average body size of an assemblage.To do this we need to grab coordinates for each assemblage, which we can do by getting the centroids of each hexbin:</p>
<pre class="r"><code>#Get centroid coordinates
centroid&lt;-st_centroid(df_shp)</code></pre>
<pre><code>## Warning in st_centroid.sf(df_shp): st_centroid assumes attributes are constant
## over geometries of x</code></pre>
<pre><code>## Warning in st_centroid.sfc(st_geometry(x), of_largest_polygon =
## of_largest_polygon): st_centroid does not give correct centroids for longitude/
## latitude data</code></pre>
<pre class="r"><code>df_shp2&lt;-cbind(df_shp,st_coordinates(centroid))
model_df&lt;-df_shp2 %&gt;% st_drop_geometry() %&gt;%
  dplyr::select(cell,geo_avg, SR, ATR, X,Y) %&gt;% drop_na() #get rid of NAs here
colnames(model_df)[5:6]&lt;-c(&quot;lon&quot;, &quot;lat&quot;) #rename X and Y as lat and lon
head(model_df)</code></pre>
<pre><code>##   cell  geo_avg SR      ATR        lon      lat
## 1    1 1.091213 56 26.58891   11.83942 58.94358
## 2    9 1.688046  7 23.71292   19.20741 69.48915
## 3   10 1.209047 50 29.55614   14.98825 64.43302
## 4   12 1.212341 15 34.34307 -156.22050 60.92071
## 5   13 1.096853 22 46.19601 -149.96528 67.06155
## 6   22 1.128458 19 38.76278 -141.02016 61.56315</code></pre>
<p>Now we have our dataset for our GLS model. Now we need to modify the gls() function to accept a spatial autocorrelation structure that accounts for the curvature of the earth. We can do this using modified code from stackoverflow:</p>
<pre class="r"><code>#setup haversine model
#### corHaversine - spatial correlation with haversine distance

# Calculates the geodesic distance between two points specified by radian latitude/longitude using Haversine formula.
# output in km
haversine &lt;- function(x0, x1, y0, y1) {
  a &lt;- sin( (y1 - y0)/2 )^2 + cos(y0) * cos(y1) * sin( (x1 - x0)/2 )^2
  v &lt;- 2 * asin( min(1, sqrt(a) ) )
  6371 * v
}

# function to compute geodesic haversine distance given two-column matrix of longitude/latitude
# input is assumed in form decimal degrees if radians = F
# note fields::rdist.earth is more efficient
haversineDist &lt;- function(xy, radians = F) {
  if (ncol(xy) &gt; 2) stop(&quot;Input must have two columns (longitude and latitude)&quot;)
  if (radians == F) xy &lt;- xy * pi/180
  hMat &lt;- matrix(NA, ncol = nrow(xy), nrow = nrow(xy))
  for (i in 1:nrow(xy) ) {
    for (j in i:nrow(xy) ) {
      hMat[j,i] &lt;- haversine(xy[i,1], xy[j,1], xy[i,2], xy[j,2]) 
    }
  }
  as.dist(hMat)
}

## for most methods, machinery from corSpatial will work without modification
Initialize.corHaversine &lt;- nlme:::Initialize.corSpatial
recalc.corHaversine &lt;- nlme:::recalc.corSpatial
Variogram.corHaversine &lt;- nlme:::Variogram.corSpatial
corFactor.corHaversine &lt;- nlme:::corFactor.corSpatial
corMatrix.corHaversine &lt;- nlme:::corMatrix.corSpatial
coef.corHaversine &lt;- nlme:::coef.corSpatial
&quot;coef&lt;-.corHaversine&quot; &lt;- nlme:::&quot;coef&lt;-.corSpatial&quot;

## Constructor for the corHaversine class
corHaversine &lt;- function(value = numeric(0), form = ~ 1, mimic = &quot;corSpher&quot;, nugget = FALSE, fixed = FALSE) {
  spClass &lt;- &quot;corHaversine&quot;
  attr(value, &quot;formula&quot;) &lt;- form
  attr(value, &quot;nugget&quot;) &lt;- nugget
  attr(value, &quot;fixed&quot;) &lt;- fixed
  attr(value, &quot;function&quot;) &lt;- mimic
  class(value) &lt;- c(spClass, &quot;corStruct&quot;)
  value
}   # end corHaversine class
environment(corHaversine) &lt;- asNamespace(&quot;nlme&quot;)

Dim.corHaversine &lt;- function(object, groups, ...) {
  if (missing(groups)) return(attr(object, &quot;Dim&quot;))
  val &lt;- Dim.corStruct(object, groups)
  val[[&quot;start&quot;]] &lt;- c(0, cumsum(val[[&quot;len&quot;]] * (val[[&quot;len&quot;]] - 1)/2)[-val[[&quot;M&quot;]]])
  ## will use third component of Dim list for spClass
  names(val)[3] &lt;- &quot;spClass&quot;
  val[[3]] &lt;- match(attr(object, &quot;function&quot;), c(&quot;corSpher&quot;, &quot;corExp&quot;, &quot;corGaus&quot;, &quot;corLin&quot;, &quot;corRatio&quot;), 0)
  val
}
environment(Dim.corHaversine) &lt;- asNamespace(&quot;nlme&quot;)

## getCovariate method for corHaversine class
getCovariate.corHaversine &lt;- function(object, form = formula(object), data) {
  if (is.null(covar &lt;- attr(object, &quot;covariate&quot;))) {          # if object lacks covariate attribute
    if (missing(data)) {                                    # if object lacks data
      stop(&quot;need data to calculate covariate&quot;)
    }
    covForm &lt;- getCovariateFormula(form)
    if (length(all.vars(covForm)) &gt; 0) {                    # if covariate present
      if (attr(terms(covForm), &quot;intercept&quot;) == 1) {       # if formula includes intercept
        covForm &lt;- eval(parse(text = paste(&quot;~&quot;, deparse(covForm[[2]]),&quot;-1&quot;,sep=&quot;&quot;)))    # remove intercept
      }
      # can only take covariates with correct names
      if (length(all.vars(covForm)) &gt; 2) stop(&quot;corHaversine can only take two covariates, &#39;lon&#39; and &#39;lat&#39;&quot;)
      if ( !all(all.vars(covForm) %in% c(&quot;lon&quot;, &quot;lat&quot;)) ) stop(&quot;covariates must be named &#39;lon&#39; and &#39;lat&#39;&quot;)
      covar &lt;- as.data.frame(unclass(model.matrix(covForm, model.frame(covForm, data, drop.unused.levels = TRUE) ) ) )
      covar &lt;- covar[,order(colnames(covar), decreasing = T)] # order as lon ... lat
    }
    else {
      covar &lt;- NULL
    }
    
    if (!is.null(getGroupsFormula(form))) {                 # if groups in formula extract covar by groups
      grps &lt;- getGroups(object, data = data)
      if (is.null(covar)) {
        covar &lt;- lapply(split(grps, grps), function(x) as.vector(dist(1:length(x) ) ) )     # filler?
      } 
      else {
        giveDist &lt;- function(el) {
          el &lt;- as.matrix(el)
          if (nrow(el) &gt; 1) as.vector(haversineDist(el))                       
          else numeric(0)
        }
        covar &lt;- lapply(split(covar, grps), giveDist )
      }
      covar &lt;- covar[sapply(covar, length) &gt; 0]  # no 1-obs groups
    } 
    else {                                  # if no groups in formula extract distance
      if (is.null(covar)) {
        covar &lt;- as.vector(dist(1:nrow(data) ) )
      } 
      else {
        covar &lt;- as.vector(haversineDist(as.matrix(covar) ) )
      }
    }
    if (any(unlist(covar) == 0)) {          # check that no distances are zero
      stop(&quot;cannot have zero distances in \&quot;corHaversine\&quot;&quot;)
    }
  }
  covar
}   # end method getCovariate
environment(getCovariate.corHaversine) &lt;- asNamespace(&quot;nlme&quot;)</code></pre>
<p>Now that we have modified the code we can run a GLS model:</p>
<pre class="r"><code>t_range1 &lt;- gls(log(geo_avg) ~ ATR, weights=varFixed(~1/SR),
                correlation = corHaversine(form=~lon+lat, mimic=&quot;corSpher&quot;), data = model_df)
summary(t_range1) #check model summary</code></pre>
<pre><code>## Generalized least squares fit by REML
##   Model: log(geo_avg) ~ ATR 
##   Data: model_df 
##         AIC       BIC   logLik
##   -447.9113 -432.9522 227.9557
## 
## Correlation Structure: corHaversine
##  Formula: ~lon + lat 
##  Parameter estimate(s):
##    range 
## 4304.542 
## Variance function:
##  Structure: fixed weights
##  Formula: ~1/SR 
## 
## Coefficients:
##                   Value   Std.Error   t-value p-value
## (Intercept) -0.17652234 0.022828624 -7.732500       0
## ATR          0.00573877 0.001044526  5.494135       0
## 
##  Correlation: 
##     (Intr)
## ATR -0.869
## 
## Standardized residuals:
##        Min         Q1        Med         Q3        Max 
## -2.1339133 -0.2530978  0.2655791  0.7634458  2.4592526 
## 
## Residual standard error: 1.841149 
## Degrees of freedom: 313 total; 311 residual</code></pre>
<pre class="r"><code>r2&lt;-cor(predict(t_range1),  log(model_df$geo_avg))^2 #caluclate R2
r2#correlation coefficient</code></pre>
<pre><code>## [1] 0.2225697</code></pre>
<p>Ok great, our model is amazing. It’s giving us significant effects that increasing ATR increases average body size of an assemblage. So perhaps harsher climates yield larger body sizes? Our correlation coefficient also tells us that ATR predicts 22% of the variation in body size!</p>
<p>Now let’s subsample. To do this we need data on every species in each hexbin. Thankfully I have this data so we just add it into the environment:</p>
<pre class="r"><code>final_cell_list&lt;-readRDS(&quot;~/Desktop/final_cell_list.rds&quot;)
head(final_cell_list)</code></pre>
<pre><code>## # A tibble: 6 x 3
## # Groups:   cell [6]
##   cell  valid_species        max_HW
##   &lt;chr&gt; &lt;chr&gt;                 &lt;dbl&gt;
## 1 502   Brachymyrmex degener  0.511
## 2 110   Brachymyrmex degener  0.511
## 3 131   Brachymyrmex degener  0.511
## 4 422   Camponotus atriceps   3.88 
## 5 413   Camponotus atriceps   3.88 
## 6 423   Camponotus atriceps   3.88</code></pre>
<p>We see that we have the species name, the size of the species, and the cell it belongs to.</p>
<p>Now we filter out cells with less than 5 species because that was the minimum threshold for an assemblage (hexbin) to be included in our dataset for analysis.</p>
<pre class="r"><code>#get cell numbers with less than 5 species
remove_cell&lt;-df_shp_master %&gt;% st_drop_geometry() %&gt;%
  filter(SR&lt;5) %&gt;%
  dplyr::select(cell)
#get species roster and headwidth per hexbin 
species_roster&lt;-final_cell_list %&gt;% 
  filter(!cell %in% remove_cell$cell) %&gt;% #gives me only unique species per hexbin
  group_by(cell, valid_species) %&gt;%
  summarise(mean = mean(max_HW, na.rm =T)) %&gt;%
  drop_na(cell)</code></pre>
<pre><code>## `summarise()` has grouped output by &#39;cell&#39;. You can override using the `.groups` argument.</code></pre>
<p>The subsampling routine is very simple:</p>
<pre class="r"><code>#ok let&#39;s run through the sampling routine
species_roster1&lt;-subset(species_roster, cell == 1) #I get the species roster for cell 1
samples&lt;-sample(1:nrow(species_roster1), 5, replace = F) #I then sample 5 random rows (species) w/o replacement from cell 1
species_roster1&lt;-species_roster1[samples,] # I then grab the randomly sampled rows from the species roster of cell 1
EnvStats::geoMean(species_roster1$mean) #then I calculate the average body size of those randomly sampled species</code></pre>
<pre><code>## [1] 0.7219455</code></pre>
<p>We just need to repeat the above for every cell and do that 100 times with this code:</p>
<pre class="r"><code>set.seed(121212)
fillthis&lt;-data.frame(cell = rep(unique(species_roster$cell),100), geo_mean = rep(rep(NA, length(unique(species_roster$cell))),100))

for(i in 1:nrow(fillthis)){
  species_roster1&lt;-subset(species_roster, cell == fillthis$cell[i])
  samples&lt;-sample(1:nrow(species_roster1), 5, replace = F)
  species_roster1&lt;-species_roster1[samples,]
  fillthis$geo_mean[i]&lt;-EnvStats::geoMean(species_roster1$mean)
}
head(fillthis)</code></pre>
<pre><code>##   cell  geo_mean
## 1    1 1.0093369
## 2   10 1.1691026
## 3  100 0.9052897
## 4  101 0.8620742
## 5  102 0.7608040
## 6  109 0.6915264</code></pre>
<p>‘Fillthis’ is now a dataframe with all our subsampling data. Essentially the total number of cells subsampled for 100 times each. Now we need to run the GLS for every iteration of subsampled hexbins:</p>
<pre class="r"><code>#run gls model for every run
fillthis$marker&lt;-rep(1:100, each = length(unique(species_roster$cell)))

#get ATR,SR, and coordinates from model_df and join it to the fillthis
model_df_subset&lt;-model_df %&gt;%
  dplyr::select(cell,ATR,SR, lon, lat)
fillthis&lt;-left_join(fillthis, model_df_subset, by = &quot;cell&quot;) %&gt;%
  drop_na()

#let&#39;s run these models now
model_output = data.frame(marker = seq(1,100, length.out =  100), 
                          effect_sz_atr = numeric(length = 100),
                          std_err_atr = numeric(length = 100),
                          p_val_atr = numeric(length = 100),
                          r2 = numeric(length = 100))

for (i in 1:nrow(model_output)) {
  gls_model &lt;- gls(log(geo_mean) ~ ATR, 
                   correlation = corHaversine(form=~lon+lat, mimic=&quot;corSpher&quot;), weights=varFixed(~1/SR),
                   data = subset(fillthis[fillthis$marker == i,])) 
  model_output$r2[i]&lt;-cor(predict(gls_model),  (subset(fillthis[fillthis$marker == i,])$geo_mean))^2
  cs &lt;- as.data.frame(summary(gls_model)$tTable)
  model_output$effect_sz_atr[i]&lt;-cs$Value[2] 
  model_output$std_err_atr[i]&lt;-cs$Std.Error[2]
  model_output$p_val_atr[i]&lt;-cs$`p-value`[2]
}

head(model_output)</code></pre>
<pre><code>##   marker effect_sz_atr std_err_atr    p_val_atr         r2
## 1      1   0.002186344 0.001519873 1.512964e-01 0.07277665
## 2      2   0.004261957 0.001607867 8.443891e-03 0.07380673
## 3      3   0.004303247 0.001616429 8.166967e-03 0.05565122
## 4      4   0.003159783 0.001727070 6.827250e-02 0.04910885
## 5      5   0.007205955 0.001585183 7.846360e-06 0.06993944
## 6      6   0.007573730 0.001633876 5.248710e-06 0.11508336</code></pre>
<p>Now let’s graph our model results:</p>
<pre class="r"><code>model_output_graph&lt;-pivot_longer(model_output, 
                                 cols = c(&quot;effect_sz_atr&quot;, &quot;std_err_atr&quot;, &quot;p_val_atr&quot;,
                                          &quot;r2&quot;), 
                                 values_to  = &quot;value&quot;) 
model_output_graph$name&lt;-as.factor(model_output_graph$name)
levels(model_output_graph$name)&lt;-c(&quot;Effect Size Temp Range&quot;, &quot;p-value&quot;, &quot;Correlation Coefficient&quot;, &quot;Standard Error&quot;)
ggplot(model_output_graph) + 
  geom_histogram(mapping = aes(x = value)) + 
  facet_wrap(~name, scales = &quot;free&quot;) +
  geom_vline(data = data.frame(xint=0.05,z=&quot;p-value&quot;), aes(xintercept = xint), linetype = &quot;dotted&quot;) +
  labs(x=&quot;&quot;, y = &quot;&quot;)</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="subsampling_files/figure-html/unnamed-chunk-12-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>We can see that from the 100 GLS models, we have a mostly significant effects of ATR on body size, we also see the correlation coefficient 7.5% which is a lot less than the actual model but that is expected as the data becomes more noisy with only 5 species being randomly sampled per bin. Overall, the subsampling gives evidence that the pattern, albeit weaker, is still present when reducing bias of possibly abundant or widespread species.</p>
<p>Let’s map the average body size based off the subsampling for the entire world:</p>
<pre class="r"><code>averages&lt;-fillthis %&gt;%
  group_by(cell) %&gt;%
  summarise(avg_bod_sz = mean(geo_mean))

df_shape2&lt;-df_shp %&gt;% left_join(.,averages, by = &quot;cell&quot;)
ggplot() +
  geom_sf(data = df_shp_master, alpha = 0.7) +
  geom_sf(data = df_shape2, aes(fill = avg_bod_sz, color = avg_bod_sz)) +
  scale_fill_viridis_c()+
  scale_color_viridis_c()+
  labs(fill = &quot;Simulated\nBody\nSize&quot;, color = &quot;Simulated\nBody\nSize&quot;) +
  theme(panel.background = element_rect(fill = &quot;black&quot;, colour = NA),
        panel.grid = element_blank())</code></pre>
<p><img src="subsampling_files/figure-html/unnamed-chunk-13-1.png" width="960" style="display: block; margin: auto;" /> We see that the subsampling visually shows the same patterns of body size.</p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
